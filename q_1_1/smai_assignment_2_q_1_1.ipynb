{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1.1 Implementing Forward-Pass on LeNet-5 Architecture using MNIST Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import numpy as np\n",
    "from scipy import signal, misc\n",
    "from mnist import MNIST\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convolutional Layer\n",
    "# Input Parameters:\n",
    "# =================\n",
    "#  layer_size: tuple consisting (depth, height, width)\n",
    "#  kernel_size: tuple consisting (number_of_kernels, inp_depth, inp_height, inp_width)\n",
    "#  ntup: tuple of number of nodes in previous layer and this layer\n",
    "#  params: directory consists of pad_len and stride ... can be extended to pass anything else\n",
    "class ConvLayer:\n",
    "    def __init__(self, layer_size, kernel_size, ntup, **params):\n",
    "        self.depth, self.height, self.width = layer_size\n",
    "        self.pad = params.get('pad', 0)       # Default Padding = 0\n",
    "        self.stride = params.get('stride', 1) # Default Stride = 1\n",
    "        if self.pad < 0:\n",
    "            print(\"Invalid padding: pad cannot be negative\")\n",
    "            sys.exit()\n",
    "\n",
    "        f = np.sqrt(6)/np.sqrt(ntup[0] + ntup[1])\n",
    "        epsilon = 1e-6\n",
    "        self.kernel = np.random.uniform(-f, f + epsilon, kernel_size)\n",
    "        self.bias = np.random.uniform(-f, f + epsilon, kernel_size[0])\n",
    "        pass\n",
    "\n",
    "    # Computes the forward pass of Conv Layer.\n",
    "    # Notation:\n",
    "    # =========\n",
    "    # N = batch_size or number of images\n",
    "    # H, W = Height and Width of input layer\n",
    "    # D = Depth of input layer\n",
    "    # K = Number of filters/kernels or depth of this conv layer\n",
    "    # K_H, K_W = kernel height and Width\n",
    "    #\n",
    "    # X: Input data of shape (N, D, H, W)\n",
    "    # kernel: Weights of shape (K, K_D, K_H, K_W)\n",
    "    # bias: Bias of each filter.\n",
    "    def forward(self, X):\n",
    "        pad_len = self.pad\n",
    "        stride = self.stride\n",
    "\n",
    "        N, D, H, W = X.shape\n",
    "        K, K_D, K_H, K_W = self.kernel.shape\n",
    "\n",
    "        conv_h = (H - K_H + 2*pad_len) // stride + 1\n",
    "        conv_w = (W - K_W + 2*pad_len) // stride + 1\n",
    "\n",
    "        # feature map of a batch\n",
    "        self.feature_map = np.zeros([N, K, conv_h, conv_w])\n",
    "\n",
    "        X_padded = np.pad(X,\n",
    "                          ((0,0), (0,0), (pad_len, pad_len), (pad_len, pad_len)),\n",
    "                          'constant')\n",
    "\n",
    "        # stride = 1\n",
    "        # Rotate kernel by 180\n",
    "        kernel_180 = np.rot90(self.kernel, 2, (2,3))\n",
    "        for img in range(N):\n",
    "            for conv_depth in range(K):\n",
    "                for inp_depth in range(D):\n",
    "                    self.feature_map[img, conv_depth] += \\\n",
    "                    signal.convolve2d(X_padded[img, inp_depth],\n",
    "                                      kernel_180[conv_depth, inp_depth],\n",
    "                                      mode='valid')\n",
    "                self.feature_map[img, conv_depth] += self.bias[conv_depth]\n",
    "\n",
    "        return self.feature_map, np.sum(np.square(self.kernel))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rectified Linear Unit (ReLU)\n",
    "# Activation function after the Convolution Layer\n",
    "class ReLULayer:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    # Computes the forward pass of ReLU Layer.\n",
    "    # X: Input data of any shape\n",
    "    def forward(self, X):\n",
    "        self.feature_map = np.maximum(X, 0)\n",
    "        return self.feature_map, 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Max Pooling Layer\n",
    "# Only reduce dimensions of height and width by a factor.\n",
    "# It does not put max filter on same input twice i.e. stride = factor = kernel_dimension\n",
    "class MaxPoolLayer:\n",
    "    def __init__(self, **params):\n",
    "        self.factor = params.get('stride', 2)\n",
    "\n",
    "    # Computes the forward pass of Max Pooling Layer.\n",
    "    # Notation:\n",
    "    # =========\n",
    "    # N = batch_size or number of images\n",
    "    # H, W = Height and Width of input layer\n",
    "    # D = Depth of input layer\n",
    "    #\n",
    "    # X: Input data of shape (N, D, H, W)\n",
    "    def forward(self, X):\n",
    "        factor = self.factor\n",
    "        N, D, H, W = X.shape\n",
    "        #assert H%factor == 0 and W%factor == 0\n",
    "        self.feature_map = X.reshape(N, D, H//factor, factor, W//factor, factor).max(axis=(3,5))\n",
    "        #assert self.feature_map.shape == (N, D, H//factor, W//factor)\n",
    "        return self.feature_map, 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fully Connected Layer\n",
    "# layer_size: number of neurons/nodes in fc layer\n",
    "# kernel_size: kernel of shape (nodes_l1 , nodes_l2)\n",
    "# where,\n",
    "#    nodes_l1: number of nodes in previous layer\n",
    "#    nodes_l2: number of nodes in this fc layer\n",
    "class FCLayer:\n",
    "    def __init__(self, layer_size, kernel_size, **params):\n",
    "        self.nodes = layer_size\n",
    "        f = np.sqrt(6)/np.sqrt(kernel_size[0] + kernel_size[1])\n",
    "        epsilon = 1e-6\n",
    "        self.kernel = np.random.uniform(-f, f + epsilon, kernel_size)\n",
    "        self.bias = np.random.uniform(-f, f + epsilon, kernel_size[1])\n",
    "        pass\n",
    "\n",
    "    # Computing the forward pass of FC Layer.\n",
    "    # X: Input data of shape (N, nodes_l1)\n",
    "    # kernel: Weight array of shape (nodes_l1, nodes_l2)\n",
    "    # bias: Biases of shape (nodes_l2)\n",
    "    def forward(self, X):\n",
    "        kernel, bias = self.kernel, self.bias\n",
    "        self.activations = np.dot(X, kernel) + bias\n",
    "        #assert self.activations.shape == (X.shape[0], bias.shape[0])\n",
    "        return self.activations, np.sum(np.square(self.kernel))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sigmoid Layer Computations.\n",
    "class SigmoidLayer:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    # X: Input data of any shape\n",
    "    def forward(self, X):\n",
    "        self.feature_map = 1.0/(1.0 + np.exp(-X))\n",
    "        return self.feature_map, 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Softmax Layer Computations.\n",
    "class SoftmaxLayer:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    # Computing the forward pass of Softmax Layer.\n",
    "    # Notation:\n",
    "    # =========\n",
    "    #    N: Batch size\n",
    "    #    C: Number of nodes in Softmax Layer or classes\n",
    "    #\n",
    "    #    X: Input data of shape (N, C)\n",
    "    #    Y: Final output of shape (N, C)\n",
    "    def forward(self, X):\n",
    "        dummy = np.exp(X)\n",
    "        self.Y = dummy / np.sum(dummy, axis=1, keepdims=True)\n",
    "        return self.Y, 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates Lenet-5 architecture\n",
    "# t_input:  True Training input of shape (N, Depth, Height, Width)\n",
    "# t_output: True Training output of shape (N, Class_Label)\n",
    "class LeNet5:\n",
    "    def __init__(self, t_input, t_output, v_input, v_output):\n",
    "        # Conv Layer-1\n",
    "        conv1 = ConvLayer((6, 28, 28), (6, 1, 5, 5), (784, 4704), pad=2, stride=1)\n",
    "        relu1 = ReLULayer()\n",
    "        \n",
    "        # Sub-sampling-1\n",
    "        pool2 = MaxPoolLayer(stride=2)\n",
    "        \n",
    "        # Conv Layer-2\n",
    "        conv3 = ConvLayer((16, 10, 10), (16, 6, 5, 5), (1176, 1600), pad=0, stride=1)\n",
    "        relu3 = ReLULayer()\n",
    "        \n",
    "        # Sub-sampling-2\n",
    "        pool4 = MaxPoolLayer(stride=2)\n",
    "        \n",
    "        # Fully Connected-1\n",
    "        fc5 = FCLayer(120, (400, 120))\n",
    "        sigmoid5 = SigmoidLayer()\n",
    "        \n",
    "        # Fully Connected-2\n",
    "        fc6 = FCLayer(84, (120, 84))\n",
    "        sigmoid6 = SigmoidLayer()\n",
    "        \n",
    "        # Final Output\n",
    "        output = FCLayer(10, (84, 10))\n",
    "        softmax = SoftmaxLayer()\n",
    "        \n",
    "        self.layers = [conv1, relu1,\n",
    "                       pool2,\n",
    "                       conv3, relu3,\n",
    "                       pool4,\n",
    "                       fc5, sigmoid5,\n",
    "                       fc6, sigmoid6,\n",
    "                       output, softmax]\n",
    "\n",
    "        self.X = t_input\n",
    "        self.Y = t_output\n",
    "        self.Xv = v_input\n",
    "        self.Yv = v_output\n",
    "\n",
    "    # Create and save image of feature maps of conv and fc layers\n",
    "    # X: Input an image of shape (1, 1, 28, 28)\n",
    "    # layers: List of layers.\n",
    "    @staticmethod\n",
    "    def generate_feature_maps(X, layers, digit, batch_string):\n",
    "        inp = X\n",
    "        size = (224,224)\n",
    "        misc.imsave(\"feature_maps/\" + digit + \"/\" + \"ainput_\" + digit + batch_string + \".jpeg\",\n",
    "                    misc.imresize(X[0][0], size))\n",
    "        conv_i = 1\n",
    "        max_i = 1\n",
    "\n",
    "        for layer in layers:\n",
    "            if isinstance(layer, FCLayer) and len(inp.shape) == 4:\n",
    "                inp, ws = layer.forward(inp.reshape(inp.shape[0],\n",
    "                                                    inp.shape[1]*inp.shape[2]*inp.shape[3]))\n",
    "            else:\n",
    "                inp, ws = layer.forward(inp)\n",
    "\n",
    "            if isinstance(layer, ReLULayer):\n",
    "                for channel in range(inp.shape[1]):\n",
    "                    misc.imsave(\"feature_maps/\" + digit + \"/\" + \"conv\" + str(conv_i) + \"_c\" +\n",
    "                                str(channel+1) + batch_string + \".jpeg\",\n",
    "                                misc.imresize(inp[0][channel], size))\n",
    "                conv_i += 1\n",
    "\n",
    "            if isinstance(layer, MaxPoolLayer):\n",
    "                for channel in range(inp.shape[1]):\n",
    "                    misc.imsave(\"feature_maps/\" + digit + \"/\" + \"maxpool\" + str(max_i) + \"_c\" +\n",
    "                                str(channel+1)+ batch_string + \".jpeg\",\n",
    "                                misc.imresize(inp[0][channel], size))\n",
    "                max_i += 1\n",
    "\n",
    "    # Computes final output of neural network by passing\n",
    "    # output of one layer to another.\n",
    "    # X: Input\n",
    "    # layers: List of layers.\n",
    "    # Out: Final output\n",
    "    @staticmethod\n",
    "    def feedForward(X, layers):\n",
    "        inp = X\n",
    "        wsum = 0\n",
    "        for layer in layers:\n",
    "            if isinstance(layer, FCLayer) and len(inp.shape) == 4:\n",
    "                inp, ws = layer.forward(inp.reshape(inp.shape[0], inp.shape[1]*inp.shape[2]*inp.shape[3]))\n",
    "            else:\n",
    "                inp, ws = layer.forward(inp)\n",
    "            wsum += ws\n",
    "        return inp, wsum\n",
    "\n",
    "    # Train the LeNet-5 (Only Forward Pass).\n",
    "    # params: parameters including \"batch_size\" (can be extended)\n",
    "    def lenet_train(self, **params):\n",
    "        batch_size  = params.get(\"batch_size\", 50) # Default 50\n",
    "        print(\"Running LeNet-5 Forward-Pass on batch_size =\", batch_size)\n",
    "        \n",
    "        X_train = self.X\n",
    "        Y_train = self.Y\n",
    "        assert X_train.shape[0] == Y_train.shape[0]\n",
    "        \n",
    "        num_batches = int(np.ceil(X_train.shape[0] / batch_size))\n",
    "        X_batches = zip(np.array_split(X_train, num_batches, axis=0),\n",
    "                        np.array_split(Y_train, num_batches, axis=0))\n",
    "\n",
    "        for x, y in X_batches:\n",
    "            predictions, weight_sum = LeNet5.feedForward(x, self.layers)\n",
    "\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoadMNISTData:\n",
    "    lim = 256.0\n",
    "\n",
    "    def __init__(self, data_path):\n",
    "        self.path = data_path\n",
    "\n",
    "    def loadData(self):\n",
    "        mndata = MNIST(self.path)\n",
    "        train_img, train_label = mndata.load_training()\n",
    "        test_img, test_label = mndata.load_testing()\n",
    "        self.train_img = np.asarray(train_img, dtype='float64') / LoadMNISTData.lim\n",
    "        self.train_label = np.asarray(train_label)\n",
    "        self.test_img = np.asarray(test_img, dtype='float64') / LoadMNISTData.lim\n",
    "        self.test_label = np.asarray(test_label)\n",
    "\n",
    "        print(\"\\nTraining Images Size (train_img):         \", self.train_img.shape)\n",
    "        print(\"Training Image Labels Size (train_label): \", self.train_label.shape)\n",
    "        print(\"Test Images Size (test_img):              \", self.test_img.shape)\n",
    "        print(\"Test Images Labels Size (test_label):     \", self.test_label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Forward Pass on MNIST dataset using LeNet-5 Architecture\n",
      "\n",
      "Loading MNIST dataset ...\n",
      "\n",
      "Training Images Size (train_img):          (60000, 784)\n",
      "Training Image Labels Size (train_label):  (60000,)\n",
      "Test Images Size (test_img):               (10000, 784)\n",
      "Test Images Labels Size (test_label):      (10000,)\n",
      "\n",
      "Training set:    (50000, 1, 28, 28) (50000, 10)\n",
      "Validation set:  (10000, 1, 28, 28) (10000, 10)\n",
      "\n",
      "Creating LeNet-5 layers ...\n",
      "Running LeNet-5 Forward-Pass on batch_size = 128\n",
      "Generating feature maps for the forward-pass\n",
      "Done !!!\n"
     ]
    }
   ],
   "source": [
    "print(\"Running Forward Pass on MNIST dataset using LeNet-5 Architecture\")\n",
    "cwd = os.getcwd()\n",
    "dataset = LoadMNISTData(cwd)\n",
    "print(\"\\nLoading MNIST dataset ...\")\n",
    "dataset.loadData()\n",
    "N = 50000\n",
    "\n",
    "X_train = dataset.train_img[range(0, N)].reshape(N, 1, 28, 28)\n",
    "Y_train = np.zeros((N, 10))\n",
    "Y_train[np.arange(N), dataset.train_label[range(0, N)]] = 1\n",
    "\n",
    "M = 10000\n",
    "X_valid = dataset.train_img[N:].reshape(M, 1, 28, 28)\n",
    "Y_valid = np.zeros((M, 10))\n",
    "Y_valid[np.arange(M), dataset.train_label[N:]] = 1\n",
    "\n",
    "print(\"\\nTraining set:   \", X_train.shape, Y_train.shape)\n",
    "print(\"Validation set: \", X_valid.shape, Y_valid.shape)\n",
    "\n",
    "### Create LeNet5 object ###\n",
    "print(\"\\nCreating LeNet-5 layers ...\")\n",
    "cnn_lenet = LeNet5(X_train, Y_train, X_valid, Y_valid)\n",
    "\n",
    "### Training LeNet5 ###\n",
    "cnn_lenet.lenet_train(batch_size=128)\n",
    "\n",
    "batch_string = \"_batch_128\"\n",
    "print(\"Generating feature maps for the forward-pass\")\n",
    "### Visualize Feature Maps of conv layers for a image of a digit ###\n",
    "for digit, index in zip(range(10),[1, 8, 16, 7, 2, 0, 18, 91, 31, 45]):\n",
    "    cnn_lenet.generate_feature_maps(X_train[index].reshape(1, 1, 28, 28),\n",
    "                                     cnn_lenet.layers,\n",
    "                                     str(digit),\n",
    "                                     batch_string)\n",
    "print(\"Done !!!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
